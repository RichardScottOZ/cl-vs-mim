{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "921e17cb",
   "metadata": {},
   "source": [
    "# Self-Attention Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e66eb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# check whether it runs in Colab\n",
    "root = \".\"\n",
    "if \"google.colab\" in sys.modules:\n",
    "    print(\"Running in Colab.\")\n",
    "    !pip3 install timm==0.5.4\n",
    "    !pip3 install matplotlib==3.7.1\n",
    "    !pip3 install scikit-learn==1.2.2\n",
    "    !pip3 install fastai==2.7.12\n",
    "    !pip3 install einops==0.6.0\n",
    "    !pip3 install gdown==4.7.1\n",
    "    !pip3 install yacs==0.1.8    \n",
    "    !git clone https://github.com/naver-ai/cl-vs-mim.git\n",
    "    root = \"./cl-vs-mim\"\n",
    "    sys.path.append(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203dfb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# check gpu env\n",
    "print(f\"Torch: {torch.__version__} \\n\" + \n",
    "      f\"Availability: {torch.cuda.is_available()}\")\n",
    "print(f\"Number: {torch.cuda.device_count()} \\n\" +\n",
    "      f\"Current device: {torch.cuda.current_device()} \\n\" +\n",
    "      f\"First device: {torch.cuda.device(0)} \\n\" +\n",
    "      f\"Device name: {torch.cuda.get_device_name(0)}\") if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff99e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import timm.data.transforms_factory as tff\n",
    "from fastai.data.external import untar_data, URLs\n",
    "from utils import subsample\n",
    "\n",
    "# use imagenette (a small subset of imagenet) as an example for simplicity\n",
    "imagenet_path = untar_data(URLs.IMAGENETTE_160)\n",
    "\n",
    "imagenet_mean = np.array([0.485, 0.456, 0.406])\n",
    "imagenet_std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "transform_test = tff.transforms_imagenet_eval(\n",
    "    img_size=224, mean=imagenet_mean, std=imagenet_std,\n",
    ")\n",
    "\n",
    "test_dir = os.path.join(imagenet_path, 'val')\n",
    "dataset_test = datasets.ImageFolder(test_dir, transform_test)\n",
    "dataset_test = torch.utils.data.Subset(\n",
    "    dataset_test, \n",
    "    subsample(dataset_test, ratio=math.pow(2,-6))  # use a subsampled batch\n",
    ")\n",
    "\n",
    "dataset_test = DataLoader(\n",
    "    dataset_test, \n",
    "    num_workers=1, \n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4532bc85",
   "metadata": {},
   "source": [
    "## Load the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba96775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MoCo\n",
    "import torch\n",
    "import models.moco as moco\n",
    "from models.moco import load_state_dict\n",
    "from utils import download\n",
    "\n",
    "name = \"moco_vit_b\" \n",
    "path = \"checkpoints/moco_vit_b.pt\"\n",
    "url = \"https://dl.fbaipublicfiles.com/moco-v3/vit-b-300ep/vit-b-300ep.pth.tar\"\n",
    "\n",
    "download(url, path, force=False)  # download checkpoints\n",
    "\n",
    "model = moco.vit_base()\n",
    "model = model.cuda()\n",
    "model = model.eval()\n",
    "\n",
    "state_dict = load_state_dict(path)\n",
    "_ = model.load_state_dict(state_dict, strict=False)\n",
    "model_moco = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f959e0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimMIM \n",
    "import gdown\n",
    "import torch\n",
    "import models.simmim as simmim\n",
    "from models.simmim import load_state_dict\n",
    "\n",
    "name = \"simmim_vit_b\"\n",
    "path = \"checkpoints/simmim_vit_b.pt\"\n",
    "url = \"https://drive.google.com/u/1/uc?id=1dJn6GYkwMIcoP3zqOEyW1_iQfpBi8UOw\"\n",
    "\n",
    "gdown.cached_download(url, path, quiet=False)  # download checkpoints\n",
    "\n",
    "model = simmim.vit_base()\n",
    "model = model.cuda()\n",
    "model = model.eval()\n",
    "\n",
    "state_dict = load_state_dict(path)\n",
    "_ = model.load_state_dict(state_dict, strict=False)\n",
    "model_simmim = model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1b1d97",
   "metadata": {},
   "source": [
    "## Attention Map Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b41b3c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualize attns with samples\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def mark_token(ax, xs, batch, i, j, color='tab:red'):\n",
    "    import matplotlib.patches as patches\n",
    "    from einops import rearrange, reduce, repeat\n",
    "    from utils import restore\n",
    "    \n",
    "    ax.imshow(rearrange(restore(xs.cpu())[batch], 'c h w -> h w c'))\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    # create a rectangle patch to visualize the query token\n",
    "    rect = patches.Rectangle((14 * i, 14 * j), 14, 14, linewidth=3, edgecolor=color, facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "def visualize_attn(ax, attns, batch, i, j, depth, head):\n",
    "    \"\"\"\n",
    "    ax: matplotlib axes\n",
    "    xs: batch\n",
    "    batch, i, j: batch coords \n",
    "    attns, depth, head: \n",
    "    \"\"\"\n",
    "    \n",
    "    attn_list = [list(attn[batch].cpu()) if attn is not None else None for attn in attns]\n",
    "    attn = torch.stack(attn_list[depth])\n",
    "    attn = attn[head]  # take head-th head\n",
    "    attn = attn[1:,1:]  # drop cls token\n",
    "    attn = attn / attn.sum(dim=-1, keepdim=True)  # normalize\n",
    "\n",
    "    attn = attn[j * 14 + i]\n",
    "    attn = attn.reshape([14, 14])\n",
    "\n",
    "    ax.imshow(attn, cmap=\"plasma\", )\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "    \n",
    "model = model_moco  # select a model\n",
    "xs, ys = next(iter(dataset_test))\n",
    "with torch.no_grad():\n",
    "    xs = xs.cuda()\n",
    "    _, _, attns, _ = model(xs)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(2, 2), dpi=200)\n",
    "mark_token(ax, xs, batch=30, i=3, j=8, color='tab:red')\n",
    "\n",
    "for i in range(14):\n",
    "    for j in range(8, 14):\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(1, 1))\n",
    "        visualize_attn(ax, attns, batch=30, i=i, j=j, depth=9, head=0)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f8f456",
   "metadata": {},
   "source": [
    "## Attention Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e0abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build off the implementation of https://github.com/sayakpaul/probing-vits\n",
    "import numpy as np\n",
    "\n",
    "def compute_distance_matrix(patch_size, num_patches, length):\n",
    "    \"\"\"Helper function to compute distance matrix.\"\"\"\n",
    "    distance_matrix = np.zeros((num_patches, num_patches))\n",
    "    for i in range(num_patches):\n",
    "        for j in range(num_patches):\n",
    "            if i == j: # zero distance\n",
    "                continue\n",
    "\n",
    "            xi, yi = (int(i/length)), (i % length)\n",
    "            xj, yj = (int(j/length)), (j % length)\n",
    "            distance_matrix[i, j] = patch_size * np.linalg.norm([xi - xj, yi - yj])\n",
    "\n",
    "    return distance_matrix\n",
    "\n",
    "def calculate_mean_attention_dist(patch_size, attention_weights):\n",
    "    \"\"\" The attention_weights shape = (batch, num_heads, num_patches, num_patches)\"\"\"\n",
    "    \n",
    "    num_patches = attention_weights.shape[-1]\n",
    "    length = int(np.sqrt(num_patches))\n",
    "    assert (length**2 == num_patches), (\"Num patches is not perfect square\")\n",
    "\n",
    "    distance_matrix = compute_distance_matrix(patch_size, num_patches, length)\n",
    "    h, w = distance_matrix.shape\n",
    "\n",
    "    distance_matrix = distance_matrix.reshape((1, 1, h, w))\n",
    "    # The attention_weights along the last axis adds to 1\n",
    "    # this is due to the fact that they are softmax of the raw logits\n",
    "    # summation of the (attention_weights * distance_matrix)\n",
    "    # should result in an average distance per token\n",
    "    mean_distances = attention_weights * distance_matrix\n",
    "    mean_distances = np.sum(mean_distances, axis=-1) # sum along last axis to get average distance per token\n",
    "    mean_distances = np.mean(mean_distances, axis=-1) # now average across all the tokens\n",
    "\n",
    "    return torch.tensor(mean_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dad3f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.utils import AverageMeter\n",
    "\n",
    "model = model_moco  # select a model\n",
    "encoder_length = len(model.blocks)  # for example 12 for ViT-B\n",
    "distances = [AverageMeter() for _ in range(encoder_length)]\n",
    "\n",
    "for idx, (xs, _) in enumerate(dataset_test):\n",
    "    xs = xs.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _, _, attns, _ = model(xs)\n",
    "        \n",
    "    for i, attn in enumerate([attn for attn in attns if attn is not None]):    \n",
    "        attn = attn[:,:,1:,1:]\n",
    "        attn = attn + 1e-32\n",
    "        attn = attn / attn.sum(dim=-1, keepdim=True)  # normalize\n",
    "        attn = attn.cpu().float().detach().numpy()\n",
    "\n",
    "        distance = calculate_mean_attention_dist(patch_size=16, attention_weights=attn)\n",
    "        distances[i].update(torch.mean(distance, dim=0))  # average w.r.t. batch\n",
    "        \n",
    "    if idx > -1:\n",
    "        break\n",
    "    \n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4.3, 4), dpi=200)\n",
    "distances = [torch.mean(distance.avg) for distance in distances]\n",
    "ax.plot(range(1, 13), distances, marker=\"o\")\n",
    "ax.set_xlabel(\"Depth\")\n",
    "ax.set_ylabel(\"Attention distance (px)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e8593d",
   "metadata": {},
   "source": [
    "## Normalized Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6150ce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "from einops import rearrange, reduce, repeat\n",
    "\n",
    "def calculate_nmi(attn): \n",
    "    \"\"\" Normalized mutual information with a return type of (batch, head) \"\"\"\n",
    "    b, h, q, k = attn.shape\n",
    "    pq = torch.ones([b, h, q]).to(attn.device)\n",
    "    pq = F.softmax(pq, dim=-1)\n",
    "    pq_ext = repeat(pq, \"b h q -> b h q k\", k=k)\n",
    "    pk = reduce(attn * pq_ext, \"b h q k -> b h k\", \"sum\")\n",
    "    pk_ext = repeat(pk, \"b h k -> b h q k\", q=q)\n",
    "    \n",
    "    mi = reduce(attn * pq_ext * torch.log(attn / pk_ext), \"b h q k -> b h\", \"sum\")\n",
    "    eq = - reduce(pq * torch.log(pq), \"b h q -> b h\", \"sum\")\n",
    "    ek = - reduce(pk * torch.log(pk), \"b h k -> b h\", \"sum\")\n",
    "    \n",
    "    nmiv = mi / torch.sqrt(eq * ek)\n",
    "    \n",
    "    return nmiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfc022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.utils import AverageMeter\n",
    "\n",
    "model = model_moco  # select a model\n",
    "encoder_length = len(model.blocks)  # 12 for ViT-B\n",
    "nmis = [AverageMeter() for _ in range(encoder_length)]\n",
    "\n",
    "for idx, (xs, _) in enumerate(dataset_test):\n",
    "    xs = xs.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _, _, attns, _ = model(xs)\n",
    "        \n",
    "    for i, attn in enumerate([attn for attn in attns if attn is not None]):\n",
    "        attn = attn[:,:,1:,1:]  # drop cls token\n",
    "        attn = attn + 1e-8\n",
    "        attn = attn / attn.sum(dim=-1, keepdim=True)  # normalize\n",
    "        attn = attn.cpu().float()\n",
    "        \n",
    "        nmi = calculate_nmi(attn)\n",
    "        nmis[i].update(torch.mean(nmi, dim=0))  # average w.r.t. batch\n",
    "        \n",
    "    if idx > -1:\n",
    "        break\n",
    "    \n",
    "    \n",
    "fig, ax = plt.subplots(1, 1, figsize=(4.3, 4), dpi=200)\n",
    "nmis = [torch.mean(nmi.avg) for nmi in nmis]\n",
    "ax.plot(range(1, 13), nmis, marker=\"o\")\n",
    "ax.set_xlabel(\"Depth\")\n",
    "ax.set_ylabel(\"Normalized MI\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cl_vs_mim",
   "language": "python",
   "name": "cl_vs_mim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
